{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688c3efb-8050-4585-b8ce-8a65c2676b11",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19472a57-6fa8-4e30-babb-5a1012d349d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from dhg import Hypergraph\n",
    "from dhg.models import HGNN\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1ff9a8-7c31-4318-aa8e-ecd454b85690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a97873-cdac-4819-ad02-0599a91dbddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, G, y, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "    start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    out = net(X, G)\n",
    "    loss = F.cross_entropy(out[train_idx], y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Time: {time.time()-start:.4f}s, Loss: {loss.item():.4f}\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252d5880-9f97-48d6-858c-c9ae35d0b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max hyperedge size: 4\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/HGNN_training/cadets_train_knn_10hop.pkl\", \"rb\") as f:\n",
    "        edge_list = pickle.load(f)\n",
    "print(\"Max hyperedge size:\", max(len(edge) for edge in edge_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d518c3-584c-4138-9e15-4e84b1d979bf",
   "metadata": {},
   "source": [
    "#### Extra Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9a6c49-0acd-4749-83b3-671e6fd7c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb72d57-04e9-44fd-a47b-b6c001995019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def segment_sum(src, index, num_segments):\n",
    "    out = torch.zeros((num_segments,) + src.shape[1:], dtype=src.dtype, device=src.device)\n",
    "    return out.index_add_(0, index, src)\n",
    "\n",
    "def segment_max(src, index, num_segments):\n",
    "    # initialize with -inf\n",
    "    out = torch.full((num_segments,) + src.shape[1:], -float(\"inf\"), dtype=src.dtype, device=src.device)\n",
    "    # scatter max via reduce: loopless with scatter_reduce if available, else manual\n",
    "    # PyTorch >=2.0: torch.scatter_reduce_ (if available)\n",
    "    try:\n",
    "        out2 = out.clone()\n",
    "        out2.scatter_reduce_(0, index.view(-1, *([1]*(src.dim()-1))).expand_as(src), src, reduce=\"amax\", include_self=True)\n",
    "        return out2\n",
    "    except Exception:\n",
    "        # Fallback: do a safe max using buckets (works well enough)\n",
    "        out = out.clone()\n",
    "        for i in range(num_segments):\n",
    "            mask = (index == i)\n",
    "            if mask.any():\n",
    "                out[i] = torch.max(src[mask], dim=0).values\n",
    "        return out\n",
    "\n",
    "def segment_softmax(scores, index, num_segments):\n",
    "    # softmax per segment i: exp(s - max_i)/sum_i exp(...)\n",
    "    max_per_seg = segment_max(scores.unsqueeze(-1), index, num_segments).squeeze(-1)\n",
    "    s_centered = scores - max_per_seg[index]\n",
    "    exp_s = torch.exp(s_centered)\n",
    "    denom = segment_sum(exp_s.unsqueeze(-1), index, num_segments).squeeze(-1)\n",
    "    return exp_s / (denom[index] + 1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b76a98-818a-4b72-92f5-3d036f84bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _HyperedgeAttentionPool_pt(nn.Module):\n",
    "    def __init__(self, in_dim, attn_dim=None, topk=None):\n",
    "        super().__init__()\n",
    "        a = attn_dim or in_dim\n",
    "        self.key = nn.Linear(in_dim, a, bias=False)\n",
    "        self.val = nn.Linear(in_dim, in_dim, bias=False)\n",
    "        self.query = nn.Parameter(torch.randn(a))\n",
    "        self.topk = topk\n",
    "\n",
    "    def forward(self, x, V2E, num_edges):\n",
    "        v_ids, e_ids = V2E[0], V2E[1]  # [E_nnz]\n",
    "        K = self.key(x)                # [N, a]\n",
    "        V = self.val(x)                # [N, d]\n",
    "        scores_full = (K[v_ids] @ self.query)  # [E_nnz]\n",
    "\n",
    "        if self.topk is not None:\n",
    "            # simple top-k per edge using Python buckets (OK on CPU; set topk=None if very big)\n",
    "            from collections import defaultdict\n",
    "            keep = torch.zeros_like(scores_full, dtype=torch.bool)\n",
    "            buckets = defaultdict(list)\n",
    "            for i, e in enumerate(e_ids.tolist()):\n",
    "                buckets[e].append(i)\n",
    "            for e, idxs in buckets.items():\n",
    "                k = min(self.topk, len(idxs))\n",
    "                s = scores_full[idxs]\n",
    "                top_idx = torch.topk(s, k=k).indices\n",
    "                for j in top_idx.tolist():\n",
    "                    keep[idxs[j]] = True\n",
    "            scores = scores_full.masked_fill(~keep, float(\"-inf\"))\n",
    "        else:\n",
    "            scores = scores_full\n",
    "\n",
    "        alpha = segment_softmax(scores, e_ids, num_edges)       # [E_nnz]\n",
    "        h_e = segment_sum(V[v_ids] * alpha.unsqueeze(-1), e_ids, num_edges)  # [M, d]\n",
    "        return h_e, alpha\n",
    "\n",
    "class AttnHGNN_DHG_pt(nn.Module):\n",
    "    def __init__(self, in_dim, hid, num_classes, edge_list, dropout=0.2, attn_dim=None, topk=None):\n",
    "        super().__init__()\n",
    "        # ---- build incidence directly from the provided edge_list\n",
    "        # edge_list: List[List[int]] or List[Tuple[int,...]]\n",
    "        edge_list = [list(map(int, e)) for e in edge_list]\n",
    "\n",
    "        node_ids, edge_ids = [], []\n",
    "        for e_id, nodes in enumerate(edge_list):\n",
    "            node_ids.extend(nodes)\n",
    "            edge_ids.extend([e_id]*len(nodes))\n",
    "\n",
    "        V2E = torch.tensor([node_ids, edge_ids], dtype=torch.long)\n",
    "        E2V = torch.stack((V2E[1], V2E[0]), dim=0)\n",
    "        M = len(edge_list)\n",
    "\n",
    "        # edge degrees for spread\n",
    "        e_ids = E2V[0]\n",
    "        edge_deg = segment_sum(torch.ones_like(e_ids, dtype=torch.float32), e_ids, M).clamp(min=1)\n",
    "\n",
    "        # register buffers so they move with .to(device)\n",
    "        self.register_buffer(\"V2E\", V2E)\n",
    "        self.register_buffer(\"E2V\", E2V)\n",
    "        self.register_buffer(\"edge_deg\", edge_deg)\n",
    "        self.M = M\n",
    "\n",
    "        self.pool1 = _HyperedgeAttentionPool_pt(in_dim, attn_dim, topk)\n",
    "        self.lin1  = nn.Linear(in_dim, hid, bias=False)\n",
    "        self.pool2 = _HyperedgeAttentionPool_pt(hid, attn_dim, topk)\n",
    "        self.lin2  = nn.Linear(hid, hid, bias=False)\n",
    "        self.cls   = nn.Linear(hid, num_classes)\n",
    "        self.act   = nn.ReLU()\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        self._last_alpha = None\n",
    "\n",
    "    def edge_to_node(self, h_e, num_nodes):\n",
    "        e_ids, v_ids = self.E2V[0], self.E2V[1]\n",
    "        contrib = h_e[e_ids] / self.edge_deg[e_ids].unsqueeze(-1)\n",
    "        return segment_sum(contrib, v_ids, num_nodes)\n",
    "\n",
    "    def forward(self, X, G):\n",
    "        N = X.size(0)\n",
    "        h_e1, a1 = self.pool1(X, self.V2E, self.M)\n",
    "        X1 = self.edge_to_node(h_e1, N)\n",
    "        X1 = self.drop(self.act(self.lin1(X1)))\n",
    "\n",
    "        h_e2, a2 = self.pool2(X1, self.V2E, self.M)\n",
    "        X2 = self.edge_to_node(h_e2, N)\n",
    "        X2 = self.drop(self.act(self.lin2(X2)))\n",
    "\n",
    "        self._last_alpha = (a1, a2)\n",
    "        return self.cls(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fab1df-cf5b-4902-b358-1cd72f3f0008",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# OLD (that errored)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# net = AttnHGNN_DHG_pt(..., G=G, ...)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# NEW: pass the same edge_list you loaded from pkl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m AttnHGNN_DHG_pt(\n\u001b[0;32m----> 6\u001b[0m     in_dim\u001b[38;5;241m=\u001b[39m\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      7\u001b[0m     hid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      8\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39mnumel()),\n\u001b[1;32m      9\u001b[0m     edge_list\u001b[38;5;241m=\u001b[39medge_list,      \u001b[38;5;66;03m# <— pass edge_list directly\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     attn_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m                 \u001b[38;5;66;03m# set e.g. 128 if edges can be very large\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# OLD (that errored)\n",
    "# net = AttnHGNN_DHG_pt(..., G=G, ...)\n",
    "\n",
    "# NEW: pass the same edge_list you loaded from pkl\n",
    "net = AttnHGNN_DHG_pt(\n",
    "    in_dim=X.shape[1],\n",
    "    hid=64,\n",
    "    num_classes=int(torch.unique(y).numel()),\n",
    "    edge_list=edge_list,      # <— pass edge_list directly\n",
    "    dropout=0.2,\n",
    "    attn_dim=None,\n",
    "    topk=None                 # set e.g. 128 if edges can be very large\n",
    ").to(torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a51f827-6611-411c-b979-26b8179b060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperedges: 362602\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of hyperedges:\", len(edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa005acb-b1bc-4dd4-bb28-aebcc2c3a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "Epoch 0, Time: 1.1969s, Loss: 6686.7256\n",
      "Epoch 1, Time: 0.0358s, Loss: 2.2500\n",
      "Epoch 2, Time: 0.0366s, Loss: 51.6000\n",
      "Epoch 3, Time: 0.0510s, Loss: 3.0745\n",
      "Epoch 4, Time: 0.0350s, Loss: 12.0310\n",
      "Epoch 5, Time: 0.0418s, Loss: 2.2915\n",
      "Epoch 6, Time: 0.0361s, Loss: 1.5634\n",
      "Epoch 7, Time: 0.0305s, Loss: 1.6510\n",
      "Epoch 8, Time: 0.0341s, Loss: 20.1193\n",
      "Epoch 9, Time: 0.0343s, Loss: 44.8789\n",
      "Epoch 10, Time: 0.0313s, Loss: 2.6398\n",
      "Epoch 11, Time: 0.0303s, Loss: 6.1614\n",
      "Epoch 12, Time: 0.0287s, Loss: 41.4561\n",
      "Epoch 13, Time: 0.0336s, Loss: 23.2552\n",
      "Epoch 14, Time: 0.0298s, Loss: 42.0183\n",
      "Epoch 15, Time: 0.0711s, Loss: 1.6584\n",
      "Epoch 16, Time: 0.0247s, Loss: 1.5142\n",
      "Epoch 17, Time: 0.0345s, Loss: 1.4677\n",
      "Epoch 18, Time: 0.0334s, Loss: 1.4213\n",
      "Epoch 19, Time: 0.0306s, Loss: 1.3924\n",
      "Epoch 20, Time: 0.0555s, Loss: 1.3765\n",
      "Epoch 21, Time: 0.0328s, Loss: 1.3472\n",
      "Epoch 22, Time: 0.0326s, Loss: 1.3342\n",
      "Epoch 23, Time: 0.0376s, Loss: 1.3271\n",
      "Epoch 24, Time: 0.0303s, Loss: 1.3071\n",
      "Epoch 25, Time: 0.0298s, Loss: 1.2905\n",
      "Epoch 26, Time: 0.0300s, Loss: 1.2967\n",
      "Epoch 27, Time: 0.0256s, Loss: 1.2659\n",
      "Epoch 28, Time: 0.0346s, Loss: 1.2510\n",
      "Epoch 29, Time: 0.0348s, Loss: 1.2604\n",
      "Epoch 30, Time: 0.0347s, Loss: 1.2334\n",
      "Epoch 31, Time: 0.0228s, Loss: 1.2110\n",
      "Epoch 32, Time: 0.0555s, Loss: 1.2079\n",
      "Epoch 33, Time: 0.0303s, Loss: 1.1952\n",
      "Epoch 34, Time: 0.0411s, Loss: 1.1844\n",
      "Epoch 35, Time: 0.0420s, Loss: 1.1745\n",
      "Epoch 36, Time: 0.0412s, Loss: 1.1836\n",
      "Epoch 37, Time: 0.0423s, Loss: 1.1572\n",
      "Epoch 38, Time: 0.0355s, Loss: 1.1288\n",
      "Epoch 39, Time: 0.0760s, Loss: 1.1459\n",
      "Epoch 40, Time: 0.0405s, Loss: 1.1231\n",
      "Epoch 41, Time: 0.0382s, Loss: 1.1008\n",
      "Epoch 42, Time: 0.0419s, Loss: 1.0801\n",
      "Epoch 43, Time: 0.0607s, Loss: 1.4481\n",
      "Epoch 44, Time: 0.0434s, Loss: 1.6345\n",
      "Epoch 45, Time: 0.0398s, Loss: 1.0384\n",
      "Epoch 46, Time: 0.0431s, Loss: 0.9988\n",
      "Epoch 47, Time: 0.0382s, Loss: 0.9934\n",
      "Epoch 48, Time: 0.0404s, Loss: 0.9880\n",
      "Epoch 49, Time: 0.0420s, Loss: 0.9443\n",
      "\n",
      "Training complete. Model saved as 'hgnn_cadets_full_training.pth'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Setup\n",
    "    set_seed(2023)\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    # --- Load your data\n",
    "    X = torch.tensor(np.load(\"/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/HGNN_training/cadets_features.npy\"), dtype=torch.float)\n",
    "    y = torch.tensor(np.load(\"/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/HGNN_training/cadets_labels.npy\"), dtype=torch.long)\n",
    "\n",
    "    num_nodes = X.shape[0]\n",
    "    G = Hypergraph(num_nodes, edge_list)\n",
    "\n",
    "    # --- Use all indices for training\n",
    "    train_idx = torch.arange(len(y), dtype=torch.long)\n",
    "\n",
    "    # --- Move to GPU\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    G = G.to(device)\n",
    "\n",
    "    # --- Define Model\n",
    "    #net = HGNN(X.shape[1], 64, torch.unique(y).numel(), use_bn=True).to(device)\n",
    "    net = AttnHGNN_DHG_pt(\n",
    "    in_dim=X.shape[1],\n",
    "    hid=64,\n",
    "    num_classes=int(torch.unique(y).numel()),\n",
    "    edge_list=edge_list,   # <-- pass edge_list, not G\n",
    "    dropout=0.2,\n",
    "    attn_dim=None,\n",
    "    topk=None\n",
    ").to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    # --- Train\n",
    "    for epoch in range(50):\n",
    "        train(net, X, G, y, train_idx, optimizer, epoch)\n",
    "\n",
    "    # --- Optional: Save model\n",
    "    torch.save(net.state_dict(), \"knn_10_hop_train_cadet_sep_2.pth\")\n",
    "    # torch.save(net.state_dict(), \"contrast_cadets_full_training_pair.pth\")\n",
    "    #torch.save(net.state_dict(), \"hgnn_cadets_full_training_HDBSCAN.pth\")\n",
    "    print(\"\\nTraining complete. Model saved as 'hgnn_cadets_full_training.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736e7e2-2819-4a3a-a5b4-beb5ac0f9be5",
   "metadata": {},
   "source": [
    "#### regular knn 10 hop and combined json evaluattion both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6e2ce1-1624-4246-8d3c-e3b1a8c8c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from dhg import Hypergraph\n",
    "from dhg.models import HGNN\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b46ebe0-4bdf-42c3-b35a-ff1f141a6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_properties(nodes, node_id, properties):\n",
    "    if node_id not in nodes:\n",
    "        nodes[node_id] = []\n",
    "    nodes[node_id].extend(properties)\n",
    "\n",
    "def update_edge_index(edges, edge_index, index):\n",
    "    for src_id, dst_id in edges:\n",
    "        src = index[src_id]\n",
    "        dst = index[dst_id]\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)\n",
    "\n",
    "def prepare_graph(df):\n",
    "    nodes, labels, edges = {}, {}, []\n",
    "    dummies = {'SUBJECT_PROCESS': 0, 'FILE_OBJECT_FILE': 1, 'FILE_OBJECT_UNIX_SOCKET': 2, \n",
    "               'UnnamedPipeObject': 3, 'NetFlowObject': 4, 'FILE_OBJECT_DIR': 5}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        action = row[\"action\"]\n",
    "        properties = [row['exec'], action] + ([row['path']] if row['path'] else [])\n",
    "        \n",
    "        actor_id = row[\"actorID\"]\n",
    "        add_node_properties(nodes, actor_id, properties)\n",
    "        labels[actor_id] = dummies[row['actor_type']]\n",
    "\n",
    "        object_id = row[\"objectID\"]\n",
    "        add_node_properties(nodes, object_id, properties)\n",
    "        labels[object_id] = dummies[row['object']]\n",
    "\n",
    "        edges.append((actor_id, object_id))\n",
    "\n",
    "    features, feat_labels, edge_index, index_map = [], [], [[], []], {}\n",
    "    for node_id, props in nodes.items():\n",
    "        features.append(props)\n",
    "        feat_labels.append(labels[node_id])\n",
    "        index_map[node_id] = len(features) - 1\n",
    "\n",
    "    update_edge_index(edges, edge_index, index_map)\n",
    "\n",
    "    return features, feat_labels, edge_index, list(index_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dffed1b4-501a-438d-bb2c-86313b3be5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributes(d,p):\n",
    "    \n",
    "    f = open(p)\n",
    "    data = [json.loads(x) for x in f if \"EVENT\" in x]\n",
    "\n",
    "    info = []\n",
    "    for x in data:\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['subject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "        try:\n",
    "            timestamp = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['timestampNanos']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "        try:\n",
    "            path2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2Path']['string']\n",
    "        except:\n",
    "            path2 = ''\n",
    "        try:\n",
    "            obj2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "            info.append({'actorID':actor,'objectID':obj2,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path2})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info.append({'actorID':actor,'objectID':obj,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path})\n",
    "\n",
    "    rdf = pd.DataFrame.from_records(info).astype(str)\n",
    "    d = d.astype(str)\n",
    "\n",
    "    return d.merge(rdf,how='inner',on=['actorID','objectID','action','timestamp']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b36a8b-1a10-4b4f-88dd-94f280134cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "f = open(\"cadets_test.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "df = add_attributes(df,\"/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/ta1-cadets-e3-official-2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc689197-7c4f-4a82-a685-84d0608dd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases, labels, edges, mapp = prepare_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbdca71-8790-4fec-a8b7-15b51f2df432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved graph structure to graph_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Run once after: phrases, labels, edges, mapp = prepare_graph(df)\n",
    "graph_data = {\n",
    "    \"phrases\": phrases,\n",
    "    \"labels\": labels,\n",
    "    \"edges\": edges,\n",
    "    \"mapp\": mapp\n",
    "}\n",
    "\n",
    "with open(\"cadet_test_graphs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph_data, f)\n",
    "\n",
    "print(\"✅ Saved graph structure to graph_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0405aca-9923-499b-a185-17f7f5dcba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/hyper_test_eval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8e525-05c0-412f-83d8-e8c514dfb61f",
   "metadata": {},
   "source": [
    "#### loading from saved pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c163e7-e877-42ad-8f27-441d3bfb3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded phrases, labels, edges, and mapp from cadet_test_graphs.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load once\n",
    "with open(\"cadet_test_graphs.pkl\", \"rb\") as f:\n",
    "    graph_data = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "phrases = graph_data[\"phrases\"]\n",
    "labels = graph_data[\"labels\"]\n",
    "edges = graph_data[\"edges\"]\n",
    "mapp = graph_data[\"mapp\"]\n",
    "\n",
    "print(\"✅ Loaded phrases, labels, edges, and mapp from cadet_test_graphs.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b818189c-6fa2-4ed2-a565-8893c4f571c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Save node features\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnodes\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      8\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcadets_test_features.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())          \u001b[38;5;66;03m# for DPHGNN\u001b[39;00m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcadets_test_x.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)                             \u001b[38;5;66;03m# optional: for PyTorch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nodes' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Save node features\n",
    "x = torch.tensor(nodes, dtype=torch.float)\n",
    "np.save(\"cadets_test_features.npy\", x.cpu().numpy())          # for DPHGNN\n",
    "torch.save(x, \"cadets_test_x.pt\")                             # optional: for PyTorch\n",
    "\n",
    "# 2. Save labels\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "np.save(\"cadets_test_labels.npy\", y.cpu().numpy())\n",
    "torch.save(y, \"cadets_test_y.pt\")\n",
    "\n",
    "# 3. Save edge_index (for baseline GCNs, optional)\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "torch.save(edge_index, \"cadets_test_edge_index.pt\")\n",
    "\n",
    "# 4. Save UUID → index map\n",
    "with open(\"cadets_test_index_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mapp, f)\n",
    "\n",
    "# 5. Build and save hyperedges based on shared exec/path token\n",
    "exec_groups = defaultdict(list)\n",
    "\n",
    "for i, tokens in enumerate(phrases):\n",
    "    for t in tokens:\n",
    "        if 'bin/' in t or t.startswith('/'):  # semantic indicator\n",
    "            exec_groups[t].append(i)\n",
    "\n",
    "# Keep hyperedges with at least 3 nodes\n",
    "hyperedges = [group for group in exec_groups.values() if len(group) >= 3]\n",
    "\n",
    "# Save hyperedge list\n",
    "with open(\"cadets_test_edge_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hyperedges, f)\n",
    "\n",
    "print(\"✅ Saved: cadets_features.npy, cadets_labels.npy, cadets_edge_list.pkl, cadets_index_map.pkl, and PyTorch versions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40161fbf-7bd8-46d8-8b1b-38a3b3b82f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGNN index 0 → Original node ID C63B54D6-3DC7-11E8-A5CB-3FA3753A265A\n",
      "HGNN index 1 → Original node ID C0E60490-62BE-7B5A-BE62-6F2DBA7BA087\n",
      "HGNN index 2 → Original node ID 0295A18C-3DC8-11E8-A5CB-3FA3753A265A\n",
      "HGNN index 3 → Original node ID 66BF9E3E-E780-9C5B-80E7-EE1A0B9C8A86\n",
      "HGNN index 4 → Original node ID 51110606-D232-9652-B2D2-3A7242968F8A\n",
      "HGNN index 5 → Original node ID 0A5174D8-80FB-4258-BB80-C79A98427D6A\n",
      "HGNN index 6 → Original node ID 9B091956-B243-9D58-83B2-D0F2D89DB317\n",
      "HGNN index 7 → Original node ID DE7FB6B3-3DC7-11E8-A5CB-3FA3753A265A\n",
      "HGNN index 8 → Original node ID 01FB76C2-3DC8-11E8-A5CB-3FA3753A265A\n",
      "HGNN index 9 → Original node ID 7A350F75-9945-425D-8599-15CEBD426F06\n"
     ]
    }
   ],
   "source": [
    "for i, node_id in list(enumerate(mapp))[:10]:\n",
    "    print(f\"HGNN index {i} → Original node ID {node_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cc0370e-cbc3-46f6-a728-21d1e7c2b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6ea24a-d4de-4447-b639-7c1d8a39a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cadets_test_knn_10hop.pkl\", \"rb\") as f:\n",
    "    edge_list_test = pickle.load(f)\n",
    "#edge_list_test = edge_list_test.t().tolist() \n",
    "#edge_list_test = edge_list_test.tolist() \n",
    "# print(type(edge_list_test))\n",
    "# print(edge_list_test[:5])  # If it's a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb338f-8cc3-4369-83db-42091de33b7c",
   "metadata": {},
   "source": [
    "#### new test with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d5044f-1f4b-4fab-ab45-2c15dd421146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded (ignoring graph buffers). Missing/unexpected keys: _IncompatibleKeys(missing_keys=['V2E', 'E2V', 'edge_deg'], unexpected_keys=[])\n",
      "Evaluation result on test set: {'accuracy': 0.7600133419036865, 'f1_score': 0.3356861185349467, 'f1_score -> average@micro': 0.7600133268378997}\n",
      "Captured attention weights for attribution (block1, block2): torch.Size([1406072]) torch.Size([1406072])\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from dhg import Hypergraph\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "\n",
    "# ---- import your attention model + helpers from wherever you defined them ----\n",
    "# from hgnn_attention import AttnHGNN_DHG_pt   # if you put the class in a module\n",
    "# (assuming AttnHGNN_DHG_pt is already in scope)\n",
    "\n",
    "def set_seed(s=2023):\n",
    "    import random\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(2023)\n",
    "\n",
    "    # If you used the pure-PyTorch (Option A) implementation during training,\n",
    "    # stick to CPU to avoid the CUDA mismatch\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load test data\n",
    "    X_test = torch.tensor(np.load(\"cadets_test_features.npy\"), dtype=torch.float32)\n",
    "    y_test = torch.tensor(np.load(\"cadets_test_labels.npy\"), dtype=torch.long)\n",
    "\n",
    "    # Choose the edge list you want for testing and load it\n",
    "    with open(\"cadets_test_edge_list_knn_dedup.pkl\", \"rb\") as f:   # <-- pick one\n",
    "        edge_list_test = pickle.load(f)\n",
    "\n",
    "    # DHG hypergraph stays as before (Evaluator expects it)\n",
    "    G_test = Hypergraph(X_test.shape[0], edge_list_test)\n",
    "\n",
    "    # --- Move to device\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    G_test = G_test.to(device)\n",
    "\n",
    "    # --- Build the SAME model class/shape you trained\n",
    "    num_classes = int(torch.unique(y_test).numel())\n",
    "    # net = AttnHGNN_DHG_pt(\n",
    "    #     in_dim=X_test.shape[1],\n",
    "    #     hid=64,\n",
    "    #     num_classes=num_classes,\n",
    "    #     edge_list=edge_list_test,   # <-- pass edge list directly\n",
    "    #     dropout=0.2,\n",
    "    #     attn_dim=None,\n",
    "    #     topk=None\n",
    "    # ).to(device)\n",
    "\n",
    "    # # --- Load weights you saved after attention training\n",
    "    # # (use the actual filename you saved; below uses your latest name)\n",
    "    # ckpt = torch.load(\"knn_10_hop_train_cadet_sep_2.pth\", map_location=device)\n",
    "    # net.load_state_dict(ckpt)\n",
    "    # net.eval()\n",
    "    net = AttnHGNN_DHG_pt(\n",
    "    in_dim=X_test.shape[1],\n",
    "    hid=64,\n",
    "    num_classes=int(torch.unique(y_test).numel()),\n",
    "    edge_list=edge_list_test,   # IMPORTANT: test edge list here\n",
    "    dropout=0.2,\n",
    "    attn_dim=None,\n",
    "    topk=None\n",
    "     ).to(device)\n",
    "\n",
    "    # --- Load checkpoint, but drop graph-dependent buffers\n",
    "    ckpt = torch.load(\"knn_10_hop_train_cadet_sep_2.pth\", map_location=device)\n",
    "    for k in [\"V2E\", \"E2V\", \"edge_deg\"]:\n",
    "        if k in ckpt: del ckpt[k]\n",
    "\n",
    "    # Load remaining (weights), letting missing keys slide\n",
    "    missing = net.load_state_dict(ckpt, strict=False)\n",
    "    print(\"Loaded (ignoring graph buffers). Missing/unexpected keys:\", missing)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "\n",
    "    # --- Inference & evaluation\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "    with torch.no_grad():\n",
    "        logits = net(X_test, G_test)\n",
    "        result = evaluator.test(y_test, logits)\n",
    "        print(\"Evaluation result on test set:\", result)\n",
    "\n",
    "    # --- Optional: get attention weights, e.g., top-α node per hyperedge (block 1)\n",
    "    if getattr(net, \"_last_alpha\", None) is not None:\n",
    "        a1, a2 = net._last_alpha\n",
    "        print(\"Captured attention weights for attribution (block1, block2):\", a1.shape, a2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e6e988-7544-4ae1-9b66-4ef0bfb577f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result on test set: {'accuracy': 0.7600133419036865, 'f1_score': 0.3356861185349467, 'f1_score -> average@micro': 0.7600133268378997}\n",
      "\n",
      "⚠️ Nodes flagged as suspicious (confidence < 0.31): 1334\n",
      "Example suspicious node IDs: [11, 17, 18, 47, 50, 168, 172, 197, 271, 272, 285, 334, 416, 683, 988, 1054, 1074, 1075, 1087, 1275]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = net(X_test, G_test)\n",
    "    result = evaluator.test(y_test, logits)\n",
    "    print(\"Evaluation result on test set:\", result)\n",
    "\n",
    "    # --- Confidence-based anomaly detection (NEW)\n",
    "    probs = F.softmax(logits, dim=1)              # class probabilities\n",
    "    conf, pred = torch.max(probs, dim=1)          # max confidence + predicted class\n",
    "    threshold = 0.31                               # you can tune this\n",
    "    low_conf_nodes = (conf < threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    print(f\"\\n⚠️ Nodes flagged as suspicious (confidence < {threshold}): {len(low_conf_nodes)}\")\n",
    "    if len(low_conf_nodes) > 0:\n",
    "        print(\"Example suspicious node IDs:\", low_conf_nodes[:20].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d892c-e392-43d3-b3f0-32786412a3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ed3a96d-d402-4726-b1bf-446d4726e45d",
   "metadata": {},
   "source": [
    "#### Old HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d75b21b-c6d6-4870-a6ba-2bf3ea321ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800395/1788347687.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"knn_10_hop_train_cadet_sep_1.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation result on test set: {'accuracy': 0.9892237186431885, 'f1_score': 0.7926043219884891, 'f1_score -> average@micro': 0.9892237396898991}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Setup\n",
    "    set_seed(2023)\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load test data\n",
    "    X_test = torch.tensor(np.load(\"cadets_test_features.npy\"), dtype=torch.float)\n",
    "    y_test = torch.tensor(np.load(\"cadets_test_labels.npy\"), dtype=torch.long)\n",
    "    # with open(\"cadets_test_edge_list.pkl\", \"rb\") as f:  #knn\n",
    "    #     edge_list_test = pickle.load(f)\n",
    "    # with open(\"cadets_test_edge_list_knn_dedup.pkl\", \"rb\") as f:  #knn\n",
    "    #     edge_list_test = pickle.load(f)\n",
    "    \n",
    "    # with open(\"cadets_test_edge_list_hdbscan_filtered.pkl\", \"rb\") as f:    #hdbscan\n",
    "    #     edge_list_test = pickle.load(f)\n",
    "\n",
    "    G_test = Hypergraph(X_test.shape[0], edge_list_test)\n",
    "\n",
    "    # --- Move data to device\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    G_test = G_test.to(device)\n",
    "\n",
    "    # --- Load trained model\n",
    "    net = HGNN(X_test.shape[1], 64, torch.unique(y_test).numel(), use_bn=True).to(device)\n",
    "    net.load_state_dict(torch.load(\"knn_10_hop_train_cadet_sep_1.pth\"))\n",
    "    #net.load_state_dict(torch.load(\"hgnn_cadets_full_training.pth\"))   ##knn model testing\n",
    "    #net.load_state_dict(torch.load(\"hgnn_cadets_full_training_HDBSCAN.pth\")) ##HDBSCSN model testing\n",
    "    net.eval()\n",
    "\n",
    "    # --- Inference & Evaluation\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "    with torch.no_grad():\n",
    "        out = net(X_test, G_test)\n",
    "        result = evaluator.test(y_test, out)\n",
    "        print(\" Evaluation result on test set:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78cb7cbb-a91d-4c52-98d1-0fd02f232f25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(y_test)\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m      4\u001b[0m tp_per_class \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "pred = out.argmax(dim=1)\n",
    "num_classes = torch.unique(y_test).numel()\n",
    "\n",
    "tp_per_class = []\n",
    "fp_per_class = []\n",
    "fn_per_class = []\n",
    "tn_per_class = []\n",
    "\n",
    "for c in range(num_classes):\n",
    "    TP = ((pred == c) & (y_test == c)).sum().item()\n",
    "    FP = ((pred == c) & (y_test != c)).sum().item()\n",
    "    FN = ((pred != c) & (y_test == c)).sum().item()\n",
    "    TN = ((pred != c) & (y_test != c)).sum().item()\n",
    "    \n",
    "    tp_per_class.append(TP)\n",
    "    fp_per_class.append(FP)\n",
    "    fn_per_class.append(FN)\n",
    "    tn_per_class.append(TN)\n",
    "\n",
    "    print(f\"\\nClass {c}:\")\n",
    "    print(f\"  TP: {TP}\")\n",
    "    print(f\"  FP: {FP}\")\n",
    "    print(f\"  FN: {FN}\")\n",
    "    print(f\"  TN: {TN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c8cce-c428-4848-a59f-b06f0e9c0328",
   "metadata": {},
   "source": [
    "#### Flash Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20abf3f5-f548-4551-b5bd-3fb33a6c8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"cadets_GT.json\", \"r\") as json_file:\n",
    "    GT_mal = set(json.load(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de8a3e-c14a-405a-891c-a0137bf8a089",
   "metadata": {},
   "source": [
    "#### Orthrus Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41289ab1-69eb-4d98-9c79-5a9cc3c49ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"combined_nodes.json\", \"r\") as json_file:\n",
    "    GT_mal = set(json.load(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "351a9de5-595f-45de-9b90-be645148b28b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     out = net(X_test, G_test)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     pred = out.argmax(dim=1)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     out = net(X_test, G_test)\n",
    "#     pred = out.argmax(dim=1)\n",
    "pred = out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed74424-4b86-411a-a913-63ea875d2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get misclassified node indices\n",
    "misclassified_nodes = (pred != y_test).nonzero(as_tuple=False).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d9011d-c011-4e07-9eaa-10a697cbdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Map misclassified indices to real node UUIDs\n",
    "predicted_malicious = set(mapp[i.item()] for i in misclassified_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c77d531b-f05e-4907-8735-9740bd1cffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cadets_GT.json\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca3bc913-45cc-40a1-8c65-d050b3bb8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Map all node indices in test set to their UUIDs\n",
    "all_nodes = set(mapp[i] for i in range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea44fe2-cf80-4ec1-a5d3-43ec8cbce51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b75084-6b81-461e-a5f8-7445838a8f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483b33d-c558-4529-939c-51af5243a853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21bd7e75-9172-4e77-8810-cb9d7a802ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth_malicious' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m misclassified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (pred \u001b[38;5;241m!=\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Also get UUIDs for correctly classified nodes that are known to be malicious\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m correct_and_malicious \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mground_truth_malicious\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#predicted_malicious = misclassified | correct_and_malicious  # expand prediction scope\u001b[39;00m\n\u001b[1;32m     13\u001b[0m predicted_malicious \u001b[38;5;241m=\u001b[39m misclassified \n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m misclassified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (pred \u001b[38;5;241m!=\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Also get UUIDs for correctly classified nodes that are known to be malicious\u001b[39;00m\n\u001b[1;32m      7\u001b[0m correct_and_malicious \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m      8\u001b[0m     mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (pred \u001b[38;5;241m==\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;129;01min\u001b[39;00m \u001b[43mground_truth_malicious\u001b[49m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#predicted_malicious = misclassified | correct_and_malicious  # expand prediction scope\u001b[39;00m\n\u001b[1;32m     13\u001b[0m predicted_malicious \u001b[38;5;241m=\u001b[39m misclassified \n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth_malicious' is not defined"
     ]
    }
   ],
   "source": [
    "pred = out.argmax(dim=1)\n",
    "\n",
    "# Get UUIDs for all predicted malicious = misclassified\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "\n",
    "# Also get UUIDs for correctly classified nodes that are known to be malicious\n",
    "correct_and_malicious = set(\n",
    "    mapp[i.item()] for i in (pred == y_test).nonzero().squeeze()\n",
    "    if mapp[i.item()] in ground_truth_malicious\n",
    ")\n",
    "\n",
    "#predicted_malicious = misclassified | correct_and_malicious  # expand prediction scope\n",
    "predicted_malicious = misclassified \n",
    "\n",
    "TP = len(predicted_malicious & ground_truth_malicious)\n",
    "FP = len(predicted_malicious - ground_truth_malicious)\n",
    "FN = len(ground_truth_malicious - predicted_malicious)\n",
    "TN = len(all_nodes - (ground_truth_malicious | predicted_malicious))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38572229-2054-454c-885c-91129e0cf587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TP: 51\n",
      "  FP: 3798\n",
      "  FN: 12807\n",
      "  TN: 340524\n"
     ]
    }
   ],
   "source": [
    "print(f\"  TP: {TP}\")\n",
    "print(f\"  FP: {FP}\")\n",
    "print(f\"  FN: {FN}\")\n",
    "print(f\"  TN: {TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abce0d1-95df-425e-98f1-ff6f9375f016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "  Precision: 0.0133\n",
      "  Recall:    0.0040\n",
      "  F1 Score:  0.0061\n"
     ]
    }
   ],
   "source": [
    "# Optional: Precision, Recall, F1\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall    = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "\n",
    "f1        = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d5b811b-0058-4a1f-aa9d-b8e7e4fe99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9535\n"
     ]
    }
   ],
   "source": [
    "# Assuming TP, FP, FN, TN are already defined\n",
    "total = TP + FP + FN + TN\n",
    "accuracy = (TP + TN) / total if total > 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0d03ddf-a82e-463b-8635-326836fe3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb3b8ce6-b527-41b2-8311-2b67a10a114a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m misclassified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of misclassified nodes (treated as malicious): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(misclassified)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu!"
     ]
    }
   ],
   "source": [
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "print(f\"Number of misclassified nodes (treated as malicious): {len(misclassified)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5397766-2257-4636-9f03-a44f7bbdcd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = len(misclassified & ground_truth_malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12cb396d-84d7-4a6b-8343-1e66e5976c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6929da-4623-4473-b0e2-a7fbcb0cdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Adjacent(ids, mapp, edges, hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    \n",
    "    neighbors = set()\n",
    "    for edge in zip(edges[0], edges[1]):\n",
    "        if any(mapp[node] in ids for node in edge):\n",
    "            neighbors.update(mapp[node] for node in edge)\n",
    "\n",
    "    if hops > 1:\n",
    "        neighbors = neighbors.union(Get_Adjacent(neighbors, mapp, edges, hops - 1))\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    FPR = FP / (FP + TN) if FP + TN > 0 else 0\n",
    "    TPR = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "\n",
    "    prec = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    rec = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    fscore = (2 * prec * rec) / (prec + rec) if prec + rec > 0 else 0\n",
    "\n",
    "    return prec, rec, fscore, FPR, TPR\n",
    "def helper(MP, all_pids, GP, edges, mapp):\n",
    "    # --- Strict FLASH logic (before 2-hop)\n",
    "    TP_raw = MP.intersection(GP)\n",
    "    FP_raw = MP - GP\n",
    "    FN_raw = GP - MP\n",
    "    TN_raw = all_pids - (GP | MP)\n",
    "\n",
    "    prec_raw, rec_raw, fscore_raw, FPR_raw, TPR_raw = calculate_metrics(\n",
    "        len(TP_raw), len(FP_raw), len(FN_raw), len(TN_raw)\n",
    "    )\n",
    "\n",
    "    print(\"📊 BEFORE 2-HOP RELAXATION\")\n",
    "    print(f\"  TP: {len(TP_raw)}, FP: {len(FP_raw)}, FN: {len(FN_raw)}, TN: {len(TN_raw)}\")\n",
    "    print(f\"  Precision: {round(prec_raw, 4)}, Recall: {round(rec_raw, 4)}, F1: {round(fscore_raw, 4)}\")\n",
    "    print(f\"  FPR: {round(FPR_raw, 4)}, TPR: {round(TPR_raw, 4)}\")\n",
    "\n",
    "    # --- Apply 2-hop relaxed logic\n",
    "    two_hop_gp = Get_Adjacent(GP, mapp, edges, 2)\n",
    "    two_hop_tp = Get_Adjacent(TP_raw, mapp, edges, 2)\n",
    "    FPL = FP_raw - two_hop_gp\n",
    "    TPL = TP_raw.union(FN_raw.intersection(two_hop_tp))\n",
    "    FN = FN_raw - two_hop_tp\n",
    "    TP, FP, TN = TPL, FPL, TN_raw  # relaxed sets\n",
    "\n",
    "    prec, rec, fscore, FPR, TPR = calculate_metrics(len(TP), len(FP), len(FN), len(TN))\n",
    "\n",
    "    print(\"\\n📊 AFTER 2-HOP RELAXATION\")\n",
    "    print(f\"  TP: {len(TP)}, FP: {len(FP)}, FN: {len(FN)}, TN: {len(TN)}\")\n",
    "    print(f\"  Precision: {round(prec, 4)}, Recall: {round(rec, 4)}, F1: {round(fscore, 4)}\")\n",
    "    print(f\"  FPR: {round(FPR, 4)}, TPR: {round(TPR, 4)}\")\n",
    "\n",
    "    return TPL, FPL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a920a-2d44-4b69-99f4-ae9f78df0713",
   "metadata": {},
   "source": [
    "#### knn - 10 hop regular - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953f41fa-c229-49d1-bf34-acbbed883378",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Get predicted class labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Misclassified node indices\u001b[39;00m\n\u001b[1;32m      5\u001b[0m misclassified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (pred \u001b[38;5;241m!=\u001b[39m y_test)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Get predicted class labels\n",
    "pred = out.argmax(dim=1)\n",
    "\n",
    "# Step 2: Misclassified node indices\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "\n",
    "# Step 3: Load ground truth malicious UUIDs\n",
    "with open(\"cadets_GT.json\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))\n",
    "\n",
    "# Step 4: Create set of all node UUIDs in the test graph\n",
    "all_node_ids = set(mapp[i] for i in range(len(y_test)))\n",
    "\n",
    "# Step 5: Run relaxed FLASH evaluation\n",
    "TPL, FPL = helper(\n",
    "    MP=misclassified,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197c4c0-d14a-4442-a332-624e6a8d6115",
   "metadata": {},
   "source": [
    "#### same approach but evaluated in combined csv/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf14b6e4-3aa4-42b3-91a5-0f9bd1125da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BEFORE 2-HOP RELAXATION\n",
      "  TP: 36, FP: 3813, FN: 36, TN: 353315\n",
      "  Precision: 0.0094, Recall: 0.5, F1: 0.0184\n",
      "  FPR: 0.0107, TPR: 0.5\n",
      "\n",
      "📊 AFTER 2-HOP RELAXATION\n",
      "  TP: 46, FP: 282, FN: 26, TN: 353315\n",
      "  Precision: 0.1402, Recall: 0.6389, F1: 0.23\n",
      "  FPR: 0.0008, TPR: 0.6389\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get predicted class labels\n",
    "pred = out.argmax(dim=1)\n",
    "\n",
    "# Step 2: Misclassified node indices\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "\n",
    "# Step 3: Load ground truth malicious UUIDs\n",
    "with open(\"combined_nodes.json\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))\n",
    "\n",
    "# Step 4: Create set of all node UUIDs in the test graph\n",
    "all_node_ids = set(mapp[i] for i in range(len(y_test)))\n",
    "\n",
    "# Step 5: Run relaxed FLASH evaluation\n",
    "TPL, FPL = helper(\n",
    "    MP=misclassified,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2316e96-68a3-407b-b778-5c6ba28caa5a",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cfd94069-7d08-4bb4-8f0b-645031fc9ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(mapp):\n\u001b[1;32m      2\u001b[0m     mapp \u001b[38;5;241m=\u001b[39m mapp\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 4\u001b[0m low_conf_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mapp[i\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlow_conf_nodes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "if torch.is_tensor(mapp):\n",
    "    mapp = mapp.detach().cpu().tolist()\n",
    "\n",
    "low_conf_nodes = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b74998be-ecc7-4bff-8a0f-e272a163552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb769c5",
   "metadata": {},
   "source": [
    "#### attention matrixevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ccd1ef7-8d69-47cb-87e8-5bfe1d60675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BEFORE 2-HOP RELAXATION\n",
      "  TP: 0, FP: 1334, FN: 72, TN: 355794\n",
      "  Precision: 0.0, Recall: 0.0, F1: 0\n",
      "  FPR: 0.0037, TPR: 0.0\n",
      "\n",
      "📊 AFTER 2-HOP RELAXATION\n",
      "  TP: 0, FP: 0, FN: 72, TN: 355794\n",
      "  Precision: 0, Recall: 0.0, F1: 0\n",
      "  FPR: 0.0, TPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Get predicted class labels\n",
    "pred = logits.argmax(dim=1)\n",
    "pred = pred.to(y_test.device)\n",
    "# MP = low_conf_nodes\n",
    "#low_conf_nodes = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())\n",
    "# # Step 2: Misclassified node indices\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "with open(\"combined_nodes.json\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))\n",
    "# Step 4: Create set of all node UUIDs in the test graph\n",
    "all_node_ids = set(mapp[i] for i in range(len(y_test)))\n",
    "\n",
    "# Step 5: Run relaxed FLASH evaluation\n",
    "TPL, FPL = helper(\n",
    "    MP=low_conf_nodes,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff335ab-8535-4b3b-858a-025efd3ce254",
   "metadata": {},
   "source": [
    "### mis plus low conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5da3c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result on test set: {'accuracy': 0.7600133419036865, 'f1_score': 0.3356861185349467, 'f1_score -> average@micro': 0.7600133268378997}\n",
      "\n",
      "⚠️ Nodes flagged as suspicious (confidence < 0.311): 1364\n",
      "Example suspicious node IDs: [11, 17, 18, 47, 50, 168, 172, 197, 271, 272, 285, 334, 416, 683, 988, 1054, 1074, 1075, 1087, 1275]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = net(X_test, G_test)\n",
    "    result = evaluator.test(y_test, logits)\n",
    "    print(\"Evaluation result on test set:\", result)\n",
    "\n",
    "    # --- Confidence-based anomaly detection (NEW)\n",
    "    probs = F.softmax(logits, dim=1)              # class probabilities\n",
    "    conf, pred = torch.max(probs, dim=1)          # max confidence + predicted class\n",
    "    threshold = 0.311                               # you can tune this\n",
    "    low_conf_nodes = (conf < threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    print(f\"\\n⚠️ Nodes flagged as suspicious (confidence < {threshold}): {len(low_conf_nodes)}\")\n",
    "    if len(low_conf_nodes) > 0:\n",
    "        print(\"Example suspicious node IDs:\", low_conf_nodes[:20].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41d411cc-e095-46fb-a6af-19a812786c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.is_tensor(mapp):\n",
    "    mapp = mapp.detach().cpu().tolist()\n",
    "low_conf_nodes = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75746b30-2e69-4b27-80e5-efecf22f2a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BEFORE 2-HOP RELAXATION\n",
      "  TP: 0, FP: 387, FN: 12858, TN: 343935\n",
      "  Precision: 0.0, Recall: 0.0, F1: 0\n",
      "  FPR: 0.0011, TPR: 0.0\n",
      "\n",
      "📊 AFTER 2-HOP RELAXATION\n",
      "  TP: 0, FP: 0, FN: 12858, TN: 343935\n",
      "  Precision: 0, Recall: 0.0, F1: 0\n",
      "  FPR: 0.0, TPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Step 1: Get predicted class labels\n",
    "# pred = logits.argmax(dim=1)\n",
    "# pred = pred.to(y_test.device)\n",
    "# MP = low_conf_nodes\n",
    "#low_conf_nodes = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())\n",
    "# # Step 2: Misclassified node indices\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "MP = misclassified & low_conf_nodes\n",
    "\n",
    "with open(\"cadets_GT.json\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))\n",
    "# with open(\"combined_nodes.json\", \"r\") as f:\n",
    "#     ground_truth_malicious = set(json.load(f))\n",
    "# Step 4: Create set of all node UUIDs in the test graph\n",
    "all_node_ids = set(mapp[i] for i in range(len(y_test)))\n",
    "\n",
    "# Step 5: Run relaxed FLASH evaluation\n",
    "TPL, FPL = helper(\n",
    "    MP=MP,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4067b98",
   "metadata": {},
   "source": [
    "#### reaper eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44cc402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BEFORE 2-HOP RELAXATION\n",
      "  TP: 3, FP: 1144, FN: 1425, TN: 355397\n",
      "  Precision: 0.0026, Recall: 0.0021, F1: 0.0023\n",
      "  FPR: 0.0032, TPR: 0.0021\n",
      "\n",
      "📊 AFTER 2-HOP RELAXATION\n",
      "  TP: 632, FP: 0, FN: 796, TN: 355397\n",
      "  Precision: 1.0, Recall: 0.4426, F1: 0.6136\n",
      "  FPR: 0.0, TPR: 0.4426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Step 1: Get predicted class labels\n",
    "pred = logits.argmax(dim=1)\n",
    "pred = pred.to(y_test.device)\n",
    "# MP = low_conf_nodes\n",
    "#low_conf_nodes = set(mapp[i.item()] for i in low_conf_nodes.detach().cpu())\n",
    "# # Step 2: Misclassified node indices\n",
    "misclassified = set(mapp[i.item()] for i in (pred != y_test).nonzero().squeeze())\n",
    "MP = misclassified & low_conf_nodes\n",
    "\n",
    "with open(\"/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/hyper_test_eval/reaper/\", \"r\") as f:\n",
    "    ground_truth_malicious = set(json.load(f))\n",
    "# with open(\"combined_nodes.json\", \"r\") as f:\n",
    "#     ground_truth_malicious = set(json.load(f))\n",
    "# Step 4: Create set of all node UUIDs in the test graph\n",
    "all_node_ids = set(mapp[i] for i in range(len(y_test)))\n",
    "\n",
    "# Step 5: Run relaxed FLASH evaluation\n",
    "TPL, FPL = helper(\n",
    "    MP=MP,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48fb52-bfb4-4c39-b593-ea52fcfb5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_conf_nodes =low_conf_nodes.to(y_test.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f822601-b02a-4453-a72d-01298e502407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Misclassified nodes (MP): 3841\n",
      "🔹 Ground truth malicious nodes (GP): 12858\n",
      "🔹 All test node UUIDs: 357174\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 Misclassified nodes (MP):\", len(misclassified))\n",
    "print(\"🔹 Ground truth malicious nodes (GP):\", len(ground_truth_malicious))\n",
    "print(\"🔹 All test node UUIDs:\", len(mapp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fec7c0a2-20bd-415e-8c6d-4be6ee8da1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Sample Misclassified: ['AE3F28A0-3E57-11E8-A5CB-3FA3753A265A', 'CBEE30D0-D58B-5DE5-A30C-74DAA6FBE323', '5B7B71CA-1921-975F-A119-1DAC3F972861', '854EEED7-3DE6-11E8-A5CB-3FA3753A265A', '72C11964-85F8-53DA-95A7-ABE856B4DEB1']\n",
      "🧾 Sample Ground Truth Malicious: ['797BB893-3E80-11E8-A5CB-3FA3753A265A', '71AAAE25-3E80-11E8-A5CB-3FA3753A265A', 'A05C089A-3E80-11E8-A5CB-3FA3753A265A', '9AF29D7C-3E80-11E8-A5CB-3FA3753A265A', '9EE588D2-3E80-11E8-A5CB-3FA3753A265A']\n",
      "🧾 Sample TP after 2-hop (TPL): ['797BB893-3E80-11E8-A5CB-3FA3753A265A', '71AAAE25-3E80-11E8-A5CB-3FA3753A265A', 'A05C089A-3E80-11E8-A5CB-3FA3753A265A', '9AF29D7C-3E80-11E8-A5CB-3FA3753A265A', '9EE588D2-3E80-11E8-A5CB-3FA3753A265A']\n",
      "🧾 Sample FN after 2-hop: ['2A03F3BA-792D-9C5B-AD79-5A5DEB9CB9D2', '0C773AFD-8F2A-3555-AA8F-AFF835357207', '4FB0BFEA-3F1C-11E8-A5CB-3FA3753A265A', 'F5B43D0A-C3F6-AE54-B6C3-B6F0B4AE4591', '885AF67E-0E96-FE5D-960E-14BA5DFEBBD3']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧾 Sample Misclassified:\", list(misclassified)[:5])\n",
    "print(\"🧾 Sample Ground Truth Malicious:\", list(ground_truth_malicious)[:5])\n",
    "print(\"🧾 Sample TP after 2-hop (TPL):\", list(TPL)[:5])\n",
    "print(\"🧾 Sample FN after 2-hop:\", list((ground_truth_malicious - TPL))[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1864bab-547c-4d59-a68d-d5905c3b71bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m two_hop_gp \u001b[38;5;241m=\u001b[39m Get_Adjacent(\u001b[43mGP\u001b[49m, mapp, edges, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m two_hop_tp \u001b[38;5;241m=\u001b[39m Get_Adjacent(TP, mapp, edges, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GP' is not defined"
     ]
    }
   ],
   "source": [
    "    two_hop_gp = Get_Adjacent(GP, mapp, edges, 2)\n",
    "    two_hop_tp = Get_Adjacent(TP, mapp, edges, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c40016a-843e-4a23-9a5c-29018c6b9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(MP, all_pids, GP, edges, mapp):\n",
    "    # --- Strict FLASH logic (before 2-hop)\n",
    "    TP_raw = MP.intersection(GP)\n",
    "    FP_raw = MP - GP\n",
    "    FN_raw = GP - MP\n",
    "    TN_raw = all_pids - (GP | MP)\n",
    "\n",
    "    prec_raw, rec_raw, fscore_raw, FPR_raw, TPR_raw = calculate_metrics(\n",
    "        len(TP_raw), len(FP_raw), len(FN_raw), len(TN_raw)\n",
    "    )\n",
    "\n",
    "    print(\"📊 BEFORE 2-HOP RELAXATION\")\n",
    "    print(f\"  TP: {len(TP_raw)}, FP: {len(FP_raw)}, FN: {len(FN_raw)}, TN: {len(TN_raw)}\")\n",
    "    print(f\"  Precision: {round(prec_raw, 4)}, Recall: {round(rec_raw, 4)}, F1: {round(fscore_raw, 4)}\")\n",
    "    print(f\"  FPR: {round(FPR_raw, 4)}, TPR: {round(TPR_raw, 4)}\")\n",
    "\n",
    "    # --- Apply 2-hop relaxed logic\n",
    "    two_hop_gp = Get_Adjacent(GP, mapp, edges, 3)\n",
    "    two_hop_tp = Get_Adjacent(TP_raw, mapp, edges, 3)\n",
    "    FPL = FP_raw - two_hop_gp\n",
    "    TPL = TP_raw.union(FN_raw.intersection(two_hop_tp))\n",
    "    FN = FN_raw - two_hop_tp\n",
    "    TP, FP, TN = TPL, FPL, TN_raw  # relaxed sets\n",
    "\n",
    "    prec, rec, fscore, FPR, TPR = calculate_metrics(len(TP), len(FP), len(FN), len(TN))\n",
    "\n",
    "    print(\"\\n📊 AFTER 2-HOP RELAXATION\")\n",
    "    print(f\"  TP: {len(TP)}, FP: {len(FP)}, FN: {len(FN)}, TN: {len(TN)}\")\n",
    "    print(f\"  Precision: {round(prec, 4)}, Recall: {round(rec, 4)}, F1: {round(fscore, 4)}\")\n",
    "    print(f\"  FPR: {round(FPR, 4)}, TPR: {round(TPR, 4)}\")\n",
    "\n",
    "    return TPL, FPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e30d55-a950-451f-983e-3bdc4ff2f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhoque2/akidul_projects/FLASH/Flash-IDS/testing/hyper_test_eval\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a1eb1a-5dfe-4d6b-a546-13ec1e990597",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_node_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m TPL, FPL \u001b[38;5;241m=\u001b[39m helper(\n\u001b[1;32m      2\u001b[0m     MP\u001b[38;5;241m=\u001b[39mmisclassified,\n\u001b[0;32m----> 3\u001b[0m     all_pids\u001b[38;5;241m=\u001b[39m\u001b[43mall_node_ids\u001b[49m,\n\u001b[1;32m      4\u001b[0m     GP\u001b[38;5;241m=\u001b[39mground_truth_malicious,\n\u001b[1;32m      5\u001b[0m     edges\u001b[38;5;241m=\u001b[39medges,  \u001b[38;5;66;03m# should be [2, num_edges] format\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     mapp\u001b[38;5;241m=\u001b[39mmapp\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_node_ids' is not defined"
     ]
    }
   ],
   "source": [
    "TPL, FPL = helper(\n",
    "    MP=misclassified,\n",
    "    all_pids=all_node_ids,\n",
    "    GP=ground_truth_malicious,\n",
    "    edges=edges,  # should be [2, num_edges] format\n",
    "    mapp=mapp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103811b-f921-48f3-a47d-014d774896a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
